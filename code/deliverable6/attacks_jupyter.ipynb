{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6f9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "#feel free to import more if you need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14fcd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the benign accuracy of a model\n",
    "def test(model, x,y,batch_size):\n",
    "    model.eval()\n",
    "    total=x.shape[0]\n",
    "    batches=np.ceil(total/batch_size).astype(int)\n",
    "    success=0\n",
    "    loss=0\n",
    "    for i in range(batches):\n",
    "        start_index=i*batch_size\n",
    "        end_index=np.minimum((i+1)*batch_size,total)\n",
    "        x_batch=torch.tensor(x[start_index:end_index]).float()\n",
    "        y_batch=torch.tensor(y[start_index:end_index]).long()\n",
    "        output=model(x_batch)\n",
    "        pred=torch.argmax(output,dim=1)\n",
    "        loss+=F.cross_entropy(output,y_batch).item()\n",
    "        success+=(pred==y_batch).sum().item()\n",
    "    #print (\"accuracy: \"+str(success/total))\n",
    "    accuracy = success/total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2855645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "        x=F.max_pool2d(x, 2)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "355f92c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#untargeted attack\n",
    "#you may add parameters as you wish\n",
    "# Reference (Recitation 3 Code)\n",
    "\n",
    "def untargeted_attack(x, y, model_1, model_2, min_val, max_val):\n",
    "    batch_size = 512\n",
    "    num_batches = (len(x) + batch_size - 1) // batch_size\n",
    "    accs = []\n",
    "    models = [model_1, model_2]\n",
    "\n",
    "    for model in models:\n",
    "        all_accs = {}\n",
    "        for eps in range(8, 64, 8):\n",
    "            epsilon = eps\n",
    "\n",
    "            adv_batch_x_all = np.zeros((len(x), x.shape[1], x.shape[2], x.shape[3]))\n",
    "            print(adv_batch_x_all.shape)\n",
    "            label_all = np.zeros((len(y),))\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, len(x))\n",
    "                batch_x = torch.tensor(x[start_idx:end_idx]).float()\n",
    "                batch_y = torch.tensor(y[start_idx:end_idx]).long()\n",
    "                adv_batch_x = batch_x.detach().clone()\n",
    "\n",
    "                for _ in range(20):\n",
    "                    adv_batch_x.requires_grad = True\n",
    "                    pred = model(adv_batch_x)\n",
    "                    #pred_B = model_2(adv_batch_x)\n",
    "                    model.zero_grad()\n",
    "                    #model_2.zero_grad()\n",
    "                    loss = -nn.CrossEntropyLoss()(pred, batch_y)\n",
    "                    loss.backward()\n",
    "                    #loss_B = -nn.CrossEntropyLoss()(pred_B, batch_y)\n",
    "                    #loss_B.backward()\n",
    "                    grads = adv_batch_x.grad\n",
    "                    with torch.no_grad():\n",
    "                        adv_batch_x = adv_batch_x - epsilon * grads.sign()\n",
    "                        eta = torch.clamp(adv_batch_x - batch_x, min=-epsilon, max=epsilon)\n",
    "                        adv_batch_x = torch.clamp(batch_x + eta, min=min_val, max=max_val).detach().clone()\n",
    "                        #adv_batch_x = torch.round(adv_batch_x*255)/255\n",
    "\n",
    "                adv_batch_x_all[start_idx:end_idx] = adv_batch_x.numpy()\n",
    "                label_all[start_idx:end_idx] = batch_y.numpy()\n",
    "\n",
    "            accuracy = test(model_1, adv_batch_x_all, label_all, 512)\n",
    "            #accuracy_B = test(model_2, adv_batch_x_all, label_all, 512)\n",
    "            all_accs[eps] = accuracy\n",
    "\n",
    "        accs.append(all_accs)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5874e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#targeted attack\n",
    "#you may add parameters as you wish\n",
    "def targeted_attack(x, y, model_1, model_2, min_val, max_val):\n",
    "    #TODO\n",
    "    \n",
    "    alpha = 10\n",
    "    batch_size = 512\n",
    "    num_batches = (len(x) + batch_size - 1) // batch_size\n",
    "    accs = []\n",
    "    models = [model_1, model_2]\n",
    "\n",
    "    for model in models:\n",
    "        all_accs = {}\n",
    "        for eps in range(8, 64, 8):\n",
    "            #epsilon = eps / 255.\n",
    "            epsilon = eps\n",
    "\n",
    "            adv_batch_x_all = np.zeros((len(x), x.shape[1], x.shape[2], x.shape[3]))\n",
    "            print(adv_batch_x_all.shape)\n",
    "            label_all = np.zeros((len(y),))\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, len(x))\n",
    "                batch_x = torch.tensor(x[start_idx:end_idx]).float()\n",
    "                batch_y = torch.tensor(y[start_idx:end_idx]).long()\n",
    "                adv_batch_x = batch_x.detach().clone()\n",
    "                target = torch.full((end_idx - start_idx,), 8, dtype=torch.long)\n",
    "                m=torch.zeros(adv_batch_x.shape)\n",
    "                v=torch.zeros(adv_batch_x.shape)\n",
    "\n",
    "                for i in range(100):\n",
    "                    adv_batch_x.requires_grad = True\n",
    "                    pred = model(adv_batch_x)\n",
    "                    model.zero_grad()\n",
    "                    loss = nn.CrossEntropyLoss()(pred, target)\n",
    "                    loss.backward()\n",
    "                    grads = adv_batch_x.grad\n",
    "                    with torch.no_grad():\n",
    "                        t=i+1\n",
    "                        m=0.9*m+0.1*grads\n",
    "                        v=0.999*v+0.001*grads*grads\n",
    "                        mhat=m/(1.0 - 0.9**t)\n",
    "                        vhat=v/(1.0 - 0.999**t)\n",
    "                        grads=mhat / (torch.sqrt(vhat) + 1e-8)\n",
    "                        adv_batch_x = adv_batch_x - alpha * grads.sign()\n",
    "                        eta = torch.clamp(adv_batch_x - batch_x, min=-epsilon, max=epsilon)\n",
    "                        adv_batch_x = torch.clamp(batch_x + eta, min=min_val, max=max_val).detach().clone()\n",
    "\n",
    "                adv_batch_x_all[start_idx:end_idx] = adv_batch_x.numpy()\n",
    "                label_all[start_idx:end_idx] = batch_y.numpy()\n",
    "\n",
    "            accuracy = test(model_1, adv_batch_x_all, label_all, 512)\n",
    "            #accuracy_B = test(model_2, adv_batch_x_all, label_all, 512)\n",
    "            all_accs[eps] = accuracy\n",
    "\n",
    "        accs.append(all_accs)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bfac6ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load MNIST\n",
    "dataset_train = datasets.MNIST('../data', train=True, download=True)\n",
    "dataset_test = datasets.MNIST('../data', train=False, download=True)\n",
    "\n",
    "# reshape MNIST\n",
    "x_train=dataset_train.data.numpy()\n",
    "y_train=dataset_train.targets.numpy()\n",
    "x_test=dataset_test.data.numpy()\n",
    "y_test=dataset_test.targets.numpy()\n",
    "x_train=np.reshape(x_train,(60000,28,28,1))\n",
    "x_test=np.reshape(x_test,(10000,28,28,1))\n",
    "x_train=np.swapaxes(x_train, 1, 3)\n",
    "x_test=np.swapaxes(x_test, 1, 3)\n",
    "\n",
    "\n",
    "#REMINDER: the range of inputs is different from what we used in the recitation\n",
    "print (x_test.min(),x_test.max())\n",
    "\n",
    "modelA=Net()\n",
    "modelA.load_state_dict(torch.load(\"modelA.zip\"))\n",
    "#accuracy_A_no_attack = test(modelA,x_test,y_test,512)\n",
    "modelB=Net()\n",
    "modelB.load_state_dict(torch.load(\"modelB.zip\"))\n",
    "#accuracy_B_no_attack = test(modelB,x_test,y_test,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "73e60b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "[{8: 0.9806167400881057, 16: 0.5612334801762114, 24: 0.000881057268722467, 32: 0.0, 40: 0.0, 48: 0.0, 56: 0.0}, {8: 0.9938325991189427, 16: 0.9920704845814978, 24: 0.986784140969163, 32: 0.9700440528634361, 40: 0.9330396475770925, 48: 0.8202643171806168, 56: 0.641409691629956}]\n"
     ]
    }
   ],
   "source": [
    "# Task - 2 - Targeted attack\n",
    "\n",
    "indices_label_1 = np.where(y_test == 1)[0]\n",
    "x_test_1s = x_test[indices_label_1]\n",
    "y_test_1s = y_test[indices_label_1]\n",
    "all_accs_task_2 = targeted_attack(x_test_1s, y_test_1s, modelA, modelB, x_test_1s.min(), x_test_1s.max())\n",
    "print(all_accs_task_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6814258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targeted_attack_improved(x, y, model_1, model_2, min_val, max_val):\n",
    "    #TODO\n",
    "    \n",
    "    #alpha = 1.5\n",
    "    #weight_decay = 1e-4\n",
    "    batch_size = 512\n",
    "    num_batches = (len(x) + batch_size - 1) // batch_size\n",
    "    accs = []\n",
    "    models = [model_1, model_2]\n",
    "\n",
    "    for model in models:\n",
    "        all_accs = {}\n",
    "        for eps in range(8, 64, 8):\n",
    "            #epsilon = eps / 255.\n",
    "            epsilon = eps\n",
    "\n",
    "            adv_batch_x_all = np.zeros((len(x), x.shape[1], x.shape[2], x.shape[3]))\n",
    "            print(adv_batch_x_all.shape)\n",
    "            label_all = np.zeros((len(y),))\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, len(x))\n",
    "                batch_x = torch.tensor(x[start_idx:end_idx]).float()\n",
    "                batch_y = torch.tensor(y[start_idx:end_idx]).long()\n",
    "                adv_batch_x = batch_x.detach().clone()\n",
    "                target = torch.full((end_idx - start_idx,), 8, dtype=torch.long)\n",
    "                m=torch.zeros(adv_batch_x.shape)\n",
    "                v=torch.zeros(adv_batch_x.shape)\n",
    "                alpha = 10\n",
    "\n",
    "                for i in range(100):\n",
    "                    adv_batch_x.requires_grad = True\n",
    "                    pred = model(adv_batch_x)\n",
    "                    model.zero_grad()\n",
    "                    loss = nn.CrossEntropyLoss()(pred, target)\n",
    "                    loss.backward()\n",
    "                    grads = adv_batch_x.grad\n",
    "                    with torch.no_grad():\n",
    "                        t=i+1\n",
    "                        m=0.9*m+0.1*grads\n",
    "                        v=0.999*v+0.001*grads*grads\n",
    "                        mhat=m/(1.0 - 0.9**t)\n",
    "                        vhat=v/(1.0 - 0.999**t)\n",
    "                        grads=mhat / (torch.sqrt(vhat) + 1e-8)\n",
    "                        adv_batch_x = adv_batch_x - alpha * grads.sign()\n",
    "                        eta = torch.clamp(adv_batch_x - batch_x, min=-epsilon, max=epsilon)\n",
    "                        adv_batch_x = torch.clamp(batch_x + eta, min=min_val, max=max_val).detach().clone()\n",
    "                    \n",
    "                    \n",
    "                    alpha = alpha / (2 ** (i/20))\n",
    "                    \n",
    "                adv_batch_x_all[start_idx:end_idx] = adv_batch_x.numpy()\n",
    "                label_all[start_idx:end_idx] = batch_y.numpy()\n",
    "\n",
    "            accuracy = test(model_1, adv_batch_x_all, label_all, 512)\n",
    "            #accuracy_B = test(model_2, adv_batch_x_all, label_all, 512)\n",
    "            all_accs[eps] = accuracy\n",
    "\n",
    "        accs.append(all_accs)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a144a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "(1135, 1, 28, 28)\n",
      "[{8: 0.9770925110132158, 16: 0.7066079295154185, 24: 0.02643171806167401, 32: 0.0, 40: 0.0, 48: 0.0, 56: 0.0}, {8: 0.9947136563876652, 16: 0.9955947136563876, 24: 0.9859030837004406, 32: 0.9770925110132158, 40: 0.9550660792951542, 48: 0.9136563876651982, 56: 0.8255506607929516}]\n"
     ]
    }
   ],
   "source": [
    "# Task - 3 - Targeted attack - Optimized\n",
    "\n",
    "all_accs_task_3 = targeted_attack_improved(x_test_1s, y_test_1s, modelA, modelB, x_test_1s.min(), x_test_1s.max())\n",
    "print(all_accs_task_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612135bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
